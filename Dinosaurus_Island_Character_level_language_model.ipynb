{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo4gVJ_D01SH"
   },
   "source": [
    "# ü¶ñ Dinosaur Name Generator  \n",
    "\n",
    "This project trains a **character-level Recurrent Neural Network (RNN)** to generate **realistic dinosaur names** based on an existing dataset. The model learns character patterns and produces new names by predicting the next character at each step.  \n",
    "\n",
    "### **Key Features**  \n",
    "‚úî Processes text data using an RNN  \n",
    "‚úî Builds a **character-level text generation model**  \n",
    "‚úî Generates novel dinosaur names through **sampling**  \n",
    "‚úî Handles **vanishing/exploding gradients** in RNNs  \n",
    "‚úî Uses **gradient clipping** for stable training  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0Nj4psY01SJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_elAxqq01SN"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Problem Statement\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB2XWVg_01SO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in the data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in the data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh3QcYpr01SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the text data, we define two mappings:  \n",
    "\n",
    "- **`char_to_ix`** ‚Üí Maps each character to an index (0-26) for training.  \n",
    "- **`ix_to_char`** ‚Üí Converts indices back to characters, allowing us to interpret the generated text.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2YltsxeZ01SU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '\\n': 0,\n",
      "    'a': 1,\n",
      "    'b': 2,\n",
      "    'c': 3,\n",
      "    'd': 4,\n",
      "    'e': 5,\n",
      "    'f': 6,\n",
      "    'g': 7,\n",
      "    'h': 8,\n",
      "    'i': 9,\n",
      "    'j': 10,\n",
      "    'k': 11,\n",
      "    'l': 12,\n",
      "    'm': 13,\n",
      "    'n': 14,\n",
      "    'o': 15,\n",
      "    'p': 16,\n",
      "    'q': 17,\n",
      "    'r': 18,\n",
      "    's': 19,\n",
      "    't': 20,\n",
      "    'u': 21,\n",
      "    'v': 22,\n",
      "    'w': 23,\n",
      "    'x': 24,\n",
      "    'y': 25,\n",
      "    'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuDQfApB01SW"
   },
   "source": [
    "### 1.2 - Model Overview  \n",
    "\n",
    "The model follows a **character-level recurrent neural network (RNN)** structure. It learns character sequences from dinosaur names and generates new names by predicting the next character at each time step.  \n",
    "\n",
    "The training loop consists of:  \n",
    "- **Initializing parameters**  \n",
    "- **Optimization loop**:\n",
    "    - Forward propagation to compute the loss function  \n",
    "    - Backward propagation to compute gradients  \n",
    "    - Clipping gradients to prevent instability  \n",
    "    - Updating parameters using gradient descent  \n",
    "- **Returning the trained parameters**  \n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:250px;\">  \n",
    "\n",
    "At each time step, the RNN predicts the next character based on previous characters:  \n",
    "\n",
    "- **$\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$** represents the input sequence (characters from the dataset).  \n",
    "- **$\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$** is the target sequence (same characters but shifted forward).  \n",
    "- At each time step $t$, the goal is to predict **$y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNFjtuyq01SW"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building Blocks of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuVLtQaM01SX"
   },
   "source": [
    "### **2.1 - Gradient Clipping in the Optimization Loop**  \n",
    "\n",
    "When training an RNN, **exploding gradients** can occur‚Äîthis happens when gradients become excessively large, causing unstable updates and making optimization difficult.  \n",
    "\n",
    "To prevent this, we apply **gradient clipping**, which ensures that gradient values stay within a predefined range **[-N, N]**. This stabilizes training by preventing large weight updates.  \n",
    "\n",
    "**Gradient Clipping Method:**  \n",
    "- Any gradient value **greater than `N`** is set to `N`.  \n",
    "- Any gradient value **less than `-N`** is set to `-N`.  \n",
    "- Values within the range **remain unchanged**.  \n",
    "\n",
    "The function below implements gradient clipping using NumPy‚Äôs `np.clip()` function. This allows us to update the gradients in-place efficiently.  \n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:600px;height:200px;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yYvYeI501SX"
   },
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out = gradient )\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxSUDRYH01Sg"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### **2.2 - Sampling**\n",
    "  \n",
    "\n",
    "To generate new dinosaur names, we use a sampling process based on our trained RNN model. The model predicts the next character step by step using the previous character as input.  \n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:750px;height:300px;\">  \n",
    "\n",
    "#### **Sampling Steps:**  \n",
    "1Ô∏è‚É£ **Initialize** with a \"dummy\" vector of zeros **$x^{\\langle 1 \\rangle} = \\vec{0}$** (before any characters are generated).  \n",
    "\n",
    "2Ô∏è‚É£ **Run a forward pass** to compute:  \n",
    "   - **Hidden state update:**  \n",
    "     $$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b) $$\n",
    "   - **Activation (logits):**  \n",
    "     $$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y $$\n",
    "   - **Softmax prediction:**  \n",
    "     $$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle }) $$\n",
    "\n",
    "3Ô∏è‚É£ **Interpret the output:**  \n",
    "   - The softmax output **$\\hat{y}^{\\langle t+1 \\rangle }$** is a probability distribution over possible next characters.  \n",
    "   - Each **$\\hat{y}^{\\langle t+1 \\rangle}_i$** represents the probability of selecting character **\"i\"** as the next character.  \n",
    "   \n",
    "4Ô∏è‚É£ **Repeat** the process until a termination condition is met (e.g., a stopping character or reaching a max length).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIkYdtBx01Su"
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix):\n",
    "    \"\"\"\n",
    "    Generates a sequence of characters based on the trained RNN model.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Dictionary containing the trained RNN parameters (Waa, Wax, Wya, by, b).\n",
    "    char_to_ix -- Dictionary mapping each character to an index.\n",
    "\n",
    "    Returns:\n",
    "    indices -- List of character indices representing the generated sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    # Initialize hidden state and input\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    x = np.zeros((vocab_size, 1))  # One-hot vector\n",
    "    \n",
    "    indices = []  # Stores the sampled character indices\n",
    "    idx = -1  # Placeholder for current character index\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    # Generate characters until newline or max length is reached\n",
    "    for _ in range(50):\n",
    "        # Forward propagation\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "\n",
    "        # Sample the next character index based on probability distribution\n",
    "        idx = np.random.choice(range(vocab_size), p=y.ravel())\n",
    "        indices.append(idx)\n",
    "\n",
    "        # Stop if newline character is generated\n",
    "        if idx == newline_character:\n",
    "            break\n",
    "\n",
    "        # Update input and hidden state\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        a_prev = a\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdeVJ9xT01S2"
   },
   "source": [
    "## 3 - Training the Language Model  \n",
    "\n",
    "### 3.1 - Gradient Descent  \n",
    "\n",
    "The model is trained using **stochastic gradient descent (SGD)**, updating weights based on one training example at a time. The optimization process follows these steps:  \n",
    "\n",
    "1Ô∏è‚É£ **Forward propagation** ‚Üí Compute the loss.  \n",
    "2Ô∏è‚É£ **Backward propagation** ‚Üí Compute gradients of the loss with respect to the parameters.  \n",
    "3Ô∏è‚É£ **Gradient clipping** ‚Üí Prevents exploding gradients by keeping values within a defined range.  \n",
    "4Ô∏è‚É£ **Parameter update** ‚Üí Apply gradient descent to adjust weights.  \n",
    "\n",
    "The following functions handle these operations:  \n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\"Performs forward propagation through the RNN and computes the loss.\"\"\"\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\"Computes gradients via backpropagation through time (BPTT).\"\"\"\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"Updates model parameters using gradient descent.\"\"\"\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BbEdIgY01S3"
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Perform one step of optimization to train the RNN model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, each representing a character in the vocabulary.\n",
    "    Y -- list of integers, same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- dictionary containing the following:\n",
    "        Wax -- Weight matrix for input-to-hidden, shape (n_a, n_x)\n",
    "        Waa -- Weight matrix for hidden-to-hidden, shape (n_a, n_a)\n",
    "        Wya -- Weight matrix for hidden-to-output, shape (n_y, n_a)\n",
    "        b -- Bias vector, shape (n_a, 1)\n",
    "        by -- Output bias vector, shape (n_y, 1)\n",
    "    learning_rate -- learning rate for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the cross-entropy loss function.\n",
    "    gradients -- dictionary containing gradients for each parameter.\n",
    "    a[len(X)-1] -- the last hidden state, shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Forward propagation\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backward propagation\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update model parameters using gradient descent\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-4cL3oZ01S9"
   },
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 - Training the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZ63mN5-01S9"
   },
   "source": [
    "### Model Implementation\n",
    "\n",
    "The `model()` function will handle the following tasks:\n",
    "\n",
    "1. **Index Management**  \n",
    "   - The index `idx` is used to cycle through the shuffled list of dinosaur names. This ensures that as you train the model, the names are fed in a random order, and after reaching the end of the list, it starts over again. The **modulo operator (`%`)** is used to achieve this cycling effect.\n",
    "\n",
    "2. **Prepare Input and Labels**  \n",
    "   - For each name in the list, convert the string of characters to a list of characters, then map those characters to their integer indices using `char_to_ix`. \n",
    "   - Prepend a **`None`** to the list of indices to create the input list `X`, which signals the start of a new sequence in the RNN.\n",
    "   - For the labels `Y`, we take the input sequence and shift it by one position to predict the next character in the sequence. We append the **newline character (`'\\n'`)** to the end of `Y`, signaling the end of the name.\n",
    "\n",
    "3. **Optimization**  \n",
    "   - The optimization loop runs for `num_iterations`, where the model iteratively adjusts parameters using the forward propagation, backward propagation, and gradient clipping steps.\n",
    "\n",
    "4. **Sampling Dinosaur Names**  \n",
    "   - Every 2000 iterations, the model generates and prints a few dinosaur names to check how well it is learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps in `model()`**\n",
    "\n",
    "1. **Preprocessing the Data**  \n",
    "   Convert the dinosaur name into a list of characters, then into indices using the `char_to_ix` mapping.\n",
    "\n",
    "2. **Input `X` and Labels `Y`**  \n",
    "   - Prepend `None` to the input list to signal the start of a sequence.\n",
    "   - The labels are the original input shifted by one character with a newline at the end.\n",
    "\n",
    "3. **Forward and Backward Passes**  \n",
    "   Use `rnn_forward` for forward propagation and `rnn_backward` for computing gradients.\n",
    "\n",
    "4. **Gradient Clipping**  \n",
    "   Use `clip` to avoid exploding gradients.\n",
    "\n",
    "5. **Update Parameters**  \n",
    "   Update the parameters using gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l214uNun01S_"
   },
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations=35000, n_a=50, dino_names=7, vocab_size=27, verbose=False):\n",
    "    \"\"\"\n",
    "    Train the model to generate dinosaur names.\n",
    "\n",
    "    Arguments:\n",
    "    data_x -- Text corpus, divided into words\n",
    "    ix_to_char -- Dictionary mapping index to character\n",
    "    char_to_ix -- Dictionary mapping character to index\n",
    "    num_iterations -- Number of training iterations\n",
    "    n_a -- Number of units in the RNN cell\n",
    "    dino_names -- Number of dinosaur names to sample each iteration\n",
    "    vocab_size -- Size of the vocabulary (number of unique characters)\n",
    "\n",
    "    Returns:\n",
    "    parameters -- Trained model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, vocab_size, vocab_size)\n",
    "    \n",
    "    # Initialize loss (used for smoothing)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Preprocess dataset\n",
    "    examples = [x.strip() for x in data_x]\n",
    "    np.random.seed()\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # Get a training example\n",
    "        idx = j % len(examples)\n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = list(single_example)\n",
    "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
    "        \n",
    "        # Prepare input (X) and labels (Y)\n",
    "        X = [None] + single_example_ix\n",
    "        Y = single_example_ix + [char_to_ix['\\n']]\n",
    "        \n",
    "        # Perform one optimization step: Forward pass -> Backward pass -> Clip gradients -> Update parameters\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=0.01)\n",
    "        \n",
    "        # Smoothing loss\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Debugging statements for tracking progress\n",
    "        if verbose and j in [0, len(examples) - 1, len(examples)]:\n",
    "            print(f\"Iteration {j}, idx {idx}\")\n",
    "        \n",
    "        # Sample dinosaur names every 2000 iterations to check progress\n",
    "        if j % 2000 == 0:\n",
    "            print(f'Iteration {j}, Loss: {loss}')\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                seed += 1\n",
    "            \n",
    "            print('\\n')\n",
    "    \n",
    "    return parameters, last_dino_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HvBlAnS01TB"
   },
   "source": [
    "### **Finally...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, idx 0\n",
      "Iteration 0, Loss: 32.96496048578151\n",
      "Ublsorxwwfhxnttkascxhqvixqupmilnsvopuojonxeqynddss\n",
      "Rcjpuneozyynyrhyysay\n",
      "Gqsarzpqalwkgbsbqyhqojafghobeogizpyxer\n",
      "Vcnarmcqltgkkjtld\n",
      "Txyksziyxoqsw\n",
      "Qlpfutgbulfjvnmvhthinwaqtfhtcqfhwdgnuxpus\n",
      "Lvnilinlzdeahssoevqyrslmvfkcmdmv\n",
      "Hdctausxxtdvfcebmwwxaun\n",
      "Mjfzbwt\n",
      "Mfwuiaxj\n",
      "\n",
      "\n",
      "Iteration 1535, idx 1535\n",
      "Iteration 1536, idx 0\n",
      "Iteration 2000, Loss: 29.326557059646476\n",
      "Atios\n",
      "Calaconghuxepcon\n",
      "Aururus\n",
      "Topqdnopcopafeyyerua\n",
      "Aussaurua\n",
      "Hhicacalus\n",
      "Paurus\n",
      "Isaierah\n",
      "Lusaurus\n",
      "Cehausiphojaurus\n",
      "\n",
      "\n",
      "Iteration 4000, Loss: 26.061546972469745\n",
      "Staniynavifidhacorhan\n",
      "Aphvidor\n",
      "Bmathystalus\n",
      "Remalus\n",
      "Feliswyophus\n",
      "Kdineerausasaus\n",
      "Abrakantausaurupr\n",
      "Acies\n",
      "Axbirdrapterusrcogiccrosihengiptanosaurus\n",
      "Audhisoranrophur\n",
      "\n",
      "\n",
      "Iteration 6000, Loss: 24.678996484891478\n",
      "Selapiasaurus\n",
      "Ritoungmasrurusklacrosaurus\n",
      "Zeuaodia\n",
      "Amasimelotctalosaurus\n",
      "Rtaraptor\n",
      "Aureendonntlepurlonothuastocavisauvus\n",
      "Usaniprosaurus\n",
      "Ontepaerotrus\n",
      "Bhusaurus\n",
      "Hechurlantrontyracrysaurus\n",
      "\n",
      "\n",
      "Iteration 8000, Loss: 24.224309160973682\n",
      "Gamiphoripalngopononicanison\n",
      "Plonis\n",
      "Ruador\n",
      "Enarobas\n",
      "Isacrosaurus\n",
      "Klanorosaurus\n",
      "Salossivus\n",
      "Lustrongoton\n",
      "Saurus\n",
      "Ansiornytix\n",
      "\n",
      "\n",
      "Iteration 10000, Loss: 23.67889925992859\n",
      "Holmika\n",
      "Waritac\n",
      "Himatisaurus\n",
      "Iudoeopterus\n",
      "Anngiceratops\n",
      "Inbmerius\n",
      "Vasmisaurus\n",
      "Onimeratopaurisaurus\n",
      "Actosaurus\n",
      "Aceptosaurus\n",
      "\n",
      "\n",
      "Iteration 12000, Loss: 23.45555865410092\n",
      "Aunasfirrosaurus\n",
      "Ticliania\n",
      "Mongcosaurunosaurus\n",
      "Onaxgsaurus\n",
      "Saurolonr\n",
      "Alicasaurus\n",
      "Hangotyratops\n",
      "Ausauraposhanranosaurus\n",
      "Ijupalosaurus\n",
      "Orianinor\n",
      "\n",
      "\n",
      "Iteration 14000, Loss: 23.427729949214466\n",
      "Sanastelonimoltalmjeus\n",
      "Toramalegnacengtanisucsaurus\n",
      "Ps\n",
      "Arpondhala\n",
      "Tastongouchpa\n",
      "Adentatptaterator\n",
      "Osucotans\n",
      "Mangsaurus\n",
      "Itutlps\n",
      "Pisaurus\n",
      "\n",
      "\n",
      "Iteration 16000, Loss: 23.096527299405057\n",
      "Laxocuxaeimanaceratops\n",
      "Stinyysaurus\n",
      "Penatirasa\n",
      "Hefmamialeus\n",
      "Roponasphaluks\n",
      "Enguoceauaumus\n",
      "Stuiosaucuratops\n",
      "Dargix\n",
      "Wchukateratoptauskuitusaurus\n",
      "Chus\n",
      "\n",
      "\n",
      "Iteration 18000, Loss: 22.970109053532695\n",
      "Buichianlathiolonlus\n",
      "Nopsaniasaurargalerntealia\n",
      "Lodeyroia\n",
      "Phostaria\n",
      "Hangaponeirapteria\n",
      "Iphiteraxopejinceritoplinkuhhamo\n",
      "Ratinia\n",
      "Salius\n",
      "Ngrangamtoraptor\n",
      "Rarangodia\n",
      "\n",
      "\n",
      "Iteration 20000, Loss: 23.012286477282984\n",
      "Angeusaurus\n",
      "Criadenosaurus\n",
      "Anchychosaurus\n",
      "Hoshindyitoria\n",
      "Chinsaurus\n",
      "Catmyinetillepsaurus\n",
      "Duangciraptia\n",
      "Burugosaurus\n",
      "Hyiraptnorianis\n",
      "Onamarnatospirritonasaurus\n",
      "\n",
      "\n",
      "Iteration 22000, Loss: 22.806701259121187\n",
      "Achorosaurus\n",
      "Innodon\n",
      "Saurus\n",
      "Phoncosaurus\n",
      "Irnithophonia\n",
      "Erasaurus\n",
      "Iaeurosaurus\n",
      "Chuanrosaurus\n",
      "Ilomadon\n",
      "Beraporasaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, dino_names = 10, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mmYZwFu01TF"
   },
   "source": [
    "### **Conclusion**\n",
    "\n",
    "As training progressed, the model began generating more plausible dinosaur names. Initially, it produced random characters, but towards the end, the output started resembling real dinosaur names with common patterns. Some cool examples include `maconucon`, `marloralus`, and `macingsersaurus`.\n",
    "\n",
    "Running the model for more iterations and experimenting with hyperparameters could lead to even better results. While some generated names may not sound as cool, remember that **not all actual dinosaur names are glamorous** (for example, `dromaeosauroides` is a real dinosaur name in the dataset). \n",
    "\n",
    "Ultimately, the model offers a set of candidate names that can be further refined or selected for coolness! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dinosaurus_Island_Character_level_language_model_final_v3b+Proposed.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "DLSC5W1-A2"
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
